# -*- coding: utf-8 -*-
"""Submmison_31_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJKDA6-3xrP6KcKrAwIq6zxMrhqXHkh6
"""

!wget --no-check-certificate \
  http://vcc.tech/file/upload_file//0/58//weboem_informations/classification.zip\
  -O /tmp/classification.zip

import zipfile,os
local_zip = '/tmp/classification.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/cuaca')
zip_ref.close()

!rm -rf /tmp/cuaca/weather_classification/cloudy/
!rm -rf /tmp/cuaca/weather_classification/haze//

import os
mypath= '/tmp/cuaca/weather_classification'
file_name = []
tag = []
full_path = []
for path, subdirs, files in os.walk(mypath):
    for name in files:
        full_path.append(os.path.join(path, name)) 
        tag.append(path.split('/')[-1])        
        file_name.append(name)

import pandas as pd
df = pd.DataFrame({"path":full_path,'file_name':file_name,"tag":tag})
df.groupby(['tag']).size()

df.head()

#load library untuk train test split
from sklearn.model_selection import train_test_split

X= df['path']
y= df['tag']

# split dataset awal menjadi data train dan test
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.20, random_state=10)

# menyatukan kedalam masing-masing dataframe

df_tr = pd.DataFrame({'path':X_train
              ,'tag':y_train
             ,'set':'train'})

df_val = pd.DataFrame({'path':X_val
              ,'tag':y_val
             ,'set':'validation'})

print('train size', len(df_tr))
print('val size', len(df_val))

# melihat proporsi pada masing masing set apakah sudah ok atau masih ada yang ingin diubah
df_all = df_tr.append([df_tr,df_val]).reset_index(drop=1)\

print('===================================================== \n')
print(df_all.groupby(['set','tag']).size(),'\n')

print('===================================================== \n')

#cek sample datanya
df_all.sample(3)

import shutil
from tqdm.notebook import tqdm as tq

datasource_path = "/tmp/cuaca/weather_classification"
dataset_path = "/tmp/cuaca/dataset"

for index, row in tq(df_all.iterrows()):
    
    
    #mendeteksi filepath
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['tag'],row['image'].split('.')[0])            
    
    #membuat direktori tujuan folder
    if os.path.exists(os.path.join(dataset_path,row['set'],row['tag'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['tag']))
    
    #tentukan tujuan file
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['tag'],destination_file_name)
    
    #menyalin file dari sumber ke tujuan
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

rainy_dir = os.path.join('/tmp/cuaca/dataset/train/rainy')
snow_dir = os.path.join('/tmp/cuaca/dataset/train/snow')
sunny_dir = os.path.join('/tmp/cuaca/dataset/train/sunny')
thunder_dir = os.path.join('/tmp/cuaca/dataset/train/thunder')


print('total training rainy images:', len(os.listdir(rainy_dir)))
print('total training snow images:', len(os.listdir(snow_dir)))
print('total training sunny images:', len(os.listdir(sunny_dir)))
print('total training thunder images:', len(os.listdir(thunder_dir)))



rainy_files = os.listdir(rainy_dir)
print(rainy_files[:10])

snow_files = os.listdir(snow_dir)
print(snow_files[:10])

sunny_files = os.listdir(sunny_dir)
print(sunny_files[:10])

thunder_files = os.listdir(thunder_dir)
print(thunder_files[:10])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

pic_index = 2

next_rainy = [os.path.join(rainy_dir, fname) 
                for fname in rainy_files[pic_index-2:pic_index]]
next_snow = [os.path.join(snow_dir, fname) 
                for fname in snow_files[pic_index-2:pic_index]]
next_sunny = [os.path.join(sunny_dir, fname) 
                for fname in sunny_files[pic_index-2:pic_index]]
next_thunder = [os.path.join(thunder_dir, fname) 
                for fname in thunder_files[pic_index-2:pic_index]]                   

for i, img_path in enumerate(next_rainy+next_snow+next_sunny+next_thunder):
  #print(img_path)
  img = mpimg.imread(img_path)
  plt.imshow(img)
  plt.axis('Off')
  plt.show()

import tensorflow as tf
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.85):
      print("\nAkurasi telah mencapai >85%!")
      self.model.stop_training = True
callbacks = myCallback()

import tensorflow as tf
import keras_preprocessing
from keras_preprocessing import image
from keras_preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout

TRAINING_DIR = "/tmp/cuaca/dataset/train/"
training_datagen = ImageDataGenerator(
      rescale = 1./255,
	    rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

VALIDATION_DIR = "/tmp/cuaca/dataset/validation/"
validation_datagen = ImageDataGenerator(
      rescale = 1./255,
      rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')



train_generator = training_datagen.flow_from_directory(
	TRAINING_DIR,
	target_size=(64,64),
	class_mode='categorical',
  batch_size=32
)

validation_generator = validation_datagen.flow_from_directory(
	VALIDATION_DIR,
	target_size=(64,64),
	class_mode='categorical',
  batch_size=32
)


model = Sequential()
model.add(InputLayer(input_shape=(64, 64, 3)))


model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=(2, 2)))


# ANN block
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
# output layer
model.add(Dense(units=4, activation='sigmoid'))
model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

history = model.fit(train_generator, epochs=25, steps_per_epoch=1000, callbacks=[callbacks], validation_data = validation_generator, verbose = 1, validation_steps=3)

model.save("cuaca.h5")

# Konversi model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs = range(len(acc))

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import numpy as np
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()
labels = ['rainy', 'snow', 'sunny','thunder']

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img = image.load_img(path, target_size=(64, 64))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=6)
  print(fn)
  print( labels[np.argmax(classes)], np.max(classes) )
  print(classes)